{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load our video file into memory\n",
    "\n",
    "Welcome to foosbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\local\\anaconda3-4.1.1-windows-x86_64\\lib\\site-packages\n",
      "Requirement already satisfied: imageio in c:\\local\\anaconda3-4.1.1-windows-x86_64\\lib\\site-packages\n",
      "Requirement already satisfied: numpy in c:\\local\\anaconda3-4.1.1-windows-x86_64\\lib\\site-packages (from imageio)\n",
      "Requirement already satisfied: pillow in c:\\local\\anaconda3-4.1.1-windows-x86_64\\lib\\site-packages (from imageio)\n",
      "Requirement already satisfied: matplotlib in c:\\local\\anaconda3-4.1.1-windows-x86_64\\lib\\site-packages\n",
      "Requirement already satisfied: numpy>=1.6 in c:\\local\\anaconda3-4.1.1-windows-x86_64\\lib\\site-packages (from matplotlib)\n",
      "Requirement already satisfied: python-dateutil in c:\\local\\anaconda3-4.1.1-windows-x86_64\\lib\\site-packages (from matplotlib)\n",
      "Requirement already satisfied: pytz in c:\\local\\anaconda3-4.1.1-windows-x86_64\\lib\\site-packages (from matplotlib)\n",
      "Requirement already satisfied: cycler in c:\\local\\anaconda3-4.1.1-windows-x86_64\\lib\\site-packages (from matplotlib)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,>=1.5.6 in c:\\local\\anaconda3-4.1.1-windows-x86_64\\lib\\site-packages (from matplotlib)\n",
      "Requirement already satisfied: six>=1.5 in c:\\local\\anaconda3-4.1.1-windows-x86_64\\lib\\site-packages (from python-dateutil->matplotlib)\n",
      "Requirement already satisfied: keras-vis in c:\\local\\anaconda3-4.1.1-windows-x86_64\\lib\\site-packages\n",
      "Requirement already satisfied: keras in c:\\local\\anaconda3-4.1.1-windows-x86_64\\lib\\site-packages (from keras-vis)\n",
      "Requirement already satisfied: scikit-image in c:\\local\\anaconda3-4.1.1-windows-x86_64\\lib\\site-packages (from keras-vis)\n",
      "Requirement already satisfied: six in c:\\local\\anaconda3-4.1.1-windows-x86_64\\lib\\site-packages (from keras-vis)\n",
      "Requirement already satisfied: pyyaml in c:\\local\\anaconda3-4.1.1-windows-x86_64\\lib\\site-packages (from keras->keras-vis)\n",
      "Requirement already satisfied: theano in c:\\local\\anaconda3-4.1.1-windows-x86_64\\lib\\site-packages (from keras->keras-vis)\n",
      "Requirement already satisfied: networkx>=1.8 in c:\\local\\anaconda3-4.1.1-windows-x86_64\\lib\\site-packages (from scikit-image->keras-vis)\n",
      "Requirement already satisfied: pillow>=2.1.0 in c:\\local\\anaconda3-4.1.1-windows-x86_64\\lib\\site-packages (from scikit-image->keras-vis)\n",
      "Requirement already satisfied: dask[array]>=0.5.0 in c:\\local\\anaconda3-4.1.1-windows-x86_64\\lib\\site-packages (from scikit-image->keras-vis)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\local\\anaconda3-4.1.1-windows-x86_64\\lib\\site-packages (from theano->keras->keras-vis)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\local\\anaconda3-4.1.1-windows-x86_64\\lib\\site-packages (from theano->keras->keras-vis)\n",
      "Requirement already satisfied: decorator>=3.4.0 in c:\\local\\anaconda3-4.1.1-windows-x86_64\\lib\\site-packages (from networkx>=1.8->scikit-image->keras-vis)\n",
      "Requirement already satisfied: toolz>=0.7.2 in c:\\local\\anaconda3-4.1.1-windows-x86_64\\lib\\site-packages (from dask[array]>=0.5.0->scikit-image->keras-vis)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "!pip install imageio\n",
    "!pip install matplotlib\n",
    "!pip install keras-vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening training frames from config .\\..\\Recorder\\FeatureSetBuilder\\Experiments\\Experiment3Result\\settings.tsv.\n",
      "Creating training chunk from .\\..\\Recorder\\FeatureSetBuilder\\Experiments\\Experiment3Result\\chunk0.avi\n",
      ".\\..\\Recorder\\FeatureSetBuilder\\Experiments\\Experiment3Result\\chunk0.avi\n",
      "added 15932 new frames for a total of 15932\n",
      "Creating training chunk from .\\..\\Recorder\\FeatureSetBuilder\\Experiments\\Experiment3Result\\chunk1.avi\n",
      ".\\..\\Recorder\\FeatureSetBuilder\\Experiments\\Experiment3Result\\chunk1.avi\n",
      "added 22943 new frames for a total of 38875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ntraining.move_first_validation_frame()\\n(frame, position) = training.get_next_validation_frame()\\ncount = 0\\nwhile frame != None:\\n    (frame, position) = training.get_next_validation_frame()\\n    print(\"%i - %i\" % (training.active_chunk, training.chunks[training.active_chunk].current_frame))\\n    count += 1\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "from random import randint\n",
    "from random import shuffle\n",
    "\n",
    "from PIL import Image\n",
    "import imageio\n",
    "import itertools as it\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(depth=6)\n",
    "\n",
    "# Paths relative to current python file.\n",
    "data_path  = \".\\\\..\\\\Recorder\\\\FeatureSetBuilder\\\\Experiments\\\\Experiment3Result\\\\settings.tsv\"\n",
    "\n",
    "class Chunk(object):\n",
    "    '''\n",
    "    Simple class that wraps video frames loaded in memory\n",
    "    '''\n",
    "\n",
    "    def __init__(self, video_file, position_file, min_positions, max_positions, position_rel_indexes, frame_rel_indexes, validation_rate):\n",
    "        # Load the position data for each frame\n",
    "        position_diff_rel_indexes = []\n",
    "        f = open(position_file)\n",
    "        self.positions = []\n",
    "        for line in f.readlines():\n",
    "            self.positions.append( list(map(int, line.split(\"\\t\")) ) )\n",
    "        f.close()\n",
    "        \n",
    "        # Normalize the position data on the range [0.0, 1.0]\n",
    "        self.positions_norm = []\n",
    "        for position in self.positions:\n",
    "            for idx, value in enumerate(position):\n",
    "                position[idx] = float(position[idx] - min_positions[idx]) / float(max_positions[idx] - min_positions[idx])\n",
    "                # TODO: BRING THESE BACK\n",
    "                #position[idx] = min(1.0, position[idx])\n",
    "                #position[idx] = max(0.0, position[idx])\n",
    "            self.positions_norm.append(position)\n",
    "        self.output_size = len(self.positions_norm[0])*len(position_rel_indexes)\n",
    "        \n",
    "        # Camera frames in memory\n",
    "        self.validation_rate = validation_rate\n",
    "        self.video_file = video_file\n",
    "        self.is_video_loaded = False\n",
    "        self.video_data = None\n",
    "        \n",
    "        print(self.video_file)\n",
    "        video_reader = imageio.get_reader(self.video_file)\n",
    "        self.num_frames   = len(video_reader)\n",
    "        first_frame = video_reader.get_data(0)\n",
    "        self.width = np.shape(first_frame)[1]\n",
    "        self.height = np.shape(first_frame)[0]\n",
    "        #pp.pprint(\"Width: %i, Height %i\" % (self.width, self.height))\n",
    "        video_reader.close()\n",
    "        \n",
    "        self.frames_training = range(round(self.num_frames * (1.0-validation_rate)) )\n",
    "        self.frames_validation = range(max(self.frames_training)+1, self.num_frames)\n",
    "        \n",
    "        \n",
    "        self.current_frame = -min(frame_rel_indexes)\n",
    "        self.position_rel_indexes = position_rel_indexes\n",
    "        self.position_diff_rel_indexes = position_diff_rel_indexes\n",
    "        self.frame_rel_indexes = frame_rel_indexes\n",
    "\n",
    "    def _load_video_memory(self):\n",
    "        # video_data: [frame#, x, y, channels]\n",
    "        if not self.is_video_loaded:\n",
    "            #print(\"Loading video into memory from %s...\" % self.video_file)\n",
    "            video_reader = imageio.get_reader(self.video_file)\n",
    "            self.video_data = np.zeros(shape=(self.num_frames, self.height, self.width, 3), dtype=np.float32)\n",
    "            for frame_index in range(0, self.num_frames):\n",
    "                self.video_data[frame_index,:,:,:] = self._read_frame(video_reader.get_data(frame_index))[:,:,:]\n",
    "            #print(\"Loaded %i frames of video into memory.\" % self.num_frames)\n",
    "            self.is_video_loaded = True\n",
    "            video_reader.close()\n",
    "            \n",
    "    def clear_video_memory(self):\n",
    "        #print(\"Clearing video memory...\")\n",
    "        #self.video_data = None\n",
    "        #self.is_video_loaded = False\n",
    "        pass\n",
    "\n",
    "    def _read_frame(self, data):\n",
    "        '''\n",
    "        Based on http://vlg.cs.dartmouth.edu/c3d/c3d_video.pdf\n",
    "        crop.\n",
    "        '''\n",
    "        image = Image.fromarray(data)\n",
    "\n",
    "        norm_image = np.array(image, dtype=np.float32)\n",
    "        norm_image -= 128.0\n",
    "        norm_image /= 128.0\n",
    "\n",
    "        # (height, width, channels)\n",
    "        return np.ascontiguousarray(norm_image)\n",
    "\n",
    "    def move_first_training_frame(self):\n",
    "        self.current_frame = -min(self.frame_rel_indexes)\n",
    "        self._load_video_memory()\n",
    "        \n",
    "    def move_first_validation_frame(self):\n",
    "        self.current_frame = min(self.frames_validation) - min(self.frame_rel_indexes)\n",
    "        self._load_video_memory()\n",
    "        \n",
    "    def move_to_frame(self, index):\n",
    "        self.current_frame = max(-min(self.frame_rel_indexes), index)\n",
    "        self._load_video_memory()\n",
    "        \n",
    "    def get_frame(self, index):\n",
    "        # Load the sequence of frames\n",
    "        frames = np.zeros(shape=(len(self.frame_rel_indexes), np.size(self.video_data,1), np.size(self.video_data,2), 3), dtype=np.float32)\n",
    "        for idx, rel_idx in enumerate(self.frame_rel_indexes):\n",
    "            frames[idx, :, :, :] = self.video_data[index + rel_idx,:,:,:]\n",
    "\n",
    "        # Load the sequence of output positions\n",
    "        output = []\n",
    "        for idx, rel_idx in enumerate(self.position_rel_indexes):\n",
    "            #pp.pprint(self.positions_norm)\n",
    "            #pp.pprint(self.positions_norm[int(index+rel_idx)][:])\n",
    "            output += list(self.positions_norm[index+rel_idx][:])\n",
    "            \n",
    "        # Load the position differences\n",
    "        \n",
    "        \n",
    "        return (frames, output)\n",
    "    \n",
    "    def get_next_training_frame(self):\n",
    "        # Returns:\n",
    "        # ([frames], [training outputs])\n",
    "        if self.is_video_loaded == True and self.current_frame + max(self.position_rel_indexes) + 1 in self.frames_training:\n",
    "            # Load the sequence of frames\n",
    "            (frames, output) = self.get_frame(self.current_frame)\n",
    "            self.current_frame += 1\n",
    "            return (frames, output)\n",
    "        else:\n",
    "            # Reached the end, clear the memory usage of this Chunk\n",
    "            self.clear_video_memory()\n",
    "            return (None, None)\n",
    "        \n",
    "    def get_next_validation_frame(self):\n",
    "         # Returns:\n",
    "        # ([frames], [training outputs])\n",
    "        if self.is_video_loaded == True and self.current_frame + max(self.position_rel_indexes) + 1 in self.frames_validation:\n",
    "            # Load the sequence of frames\n",
    "            (frames, output) = self.get_frame(self.current_frame)\n",
    "            self.current_frame += 1\n",
    "            return (frames, output)\n",
    "        else:\n",
    "            # Reached the end, clear the memory usage of this Chunk\n",
    "            self.clear_video_memory()\n",
    "            return (None, None)\n",
    "\n",
    "    \n",
    "class TrainingInput(object):\n",
    "    def __init__(self, settings_file, position_rel_indexes, frame_rel_indexes, valdiation_rate):\n",
    "        self.base_path = os.path.dirname(settings_file)\n",
    "        \n",
    "        # Create the chunks\n",
    "        f = open(settings_file,\"r\")\n",
    "        self.chunks = []\n",
    "        self.length = 0\n",
    "        self.valdiation_rate = valdiation_rate\n",
    "        self.width = None\n",
    "        self.height = None\n",
    "        self.depth = len(frame_rel_indexes)\n",
    "        self.channels = 3\n",
    "        self.output_size = None\n",
    "        for row in f.readlines():\n",
    "            tokens = row.replace(\"\\n\",\"\").split(\"\\t\")\n",
    "            num_columns = len(tokens[2:])\n",
    "            \n",
    "            min_range = list(map(int, tokens[2:int(2+num_columns/2)]))\n",
    "            max_range = list(map(int, tokens[int(2+num_columns/2):]))\n",
    "            \n",
    "            print(\"Creating training chunk from %s\" % os.path.join(self.base_path, tokens[0]))\n",
    "            chunk = Chunk(os.path.join(self.base_path, tokens[0]), os.path.join(self.base_path, tokens[1]), min_range, max_range, position_rel_indexes, frame_rel_indexes, valdiation_rate)\n",
    "            self.length += chunk.num_frames\n",
    "            self.width = chunk.width\n",
    "            self.height = chunk.height\n",
    "            self.output_size = chunk.output_size\n",
    "            \n",
    "            print(\"added %i new frames for a total of %i\" % (chunk.num_frames, self.length))\n",
    "            self.chunks.append(chunk)\n",
    "        \n",
    "        self.active_chunk = 0\n",
    "    \n",
    "    def move_first_training_frame(self):\n",
    "        self.active_chunk = 0\n",
    "        \n",
    "        for chunk in self.chunks:\n",
    "            chunk.clear_video_memory()\n",
    "        \n",
    "        if len(self.chunks) > 0 :\n",
    "            self.chunks[0].move_first_training_frame()\n",
    "            \n",
    "    def move_first_validation_frame(self):\n",
    "        self.active_chunk = 0\n",
    "        \n",
    "        for chunk in self.chunks:\n",
    "            chunk.clear_video_memory()\n",
    "        \n",
    "        if len(self.chunks) > 0 :\n",
    "            self.chunks[0].move_first_validation_frame()\n",
    "    \n",
    "    def get_next_training_frame(self):\n",
    "        if self.active_chunk < len(self.chunks):\n",
    "            # Get the next training frame from the active chunk\n",
    "            (frames, output) = self.chunks[self.active_chunk].get_next_training_frame()\n",
    "            if frames == None:\n",
    "                # Move to the next chunk\n",
    "                self.active_chunk += 1\n",
    "                if self.active_chunk < len(self.chunks):\n",
    "                    self.chunks[self.active_chunk].move_first_training_frame()\n",
    "                    return self.get_next_training_frame()\n",
    "                \n",
    "            return (frames, output)\n",
    "        \n",
    "        return (None, None)\n",
    "        \n",
    "    def get_next_validation_frame(self):\n",
    "        if self.active_chunk < len(self.chunks):\n",
    "            # Get the next training frame from the active chunk\n",
    "            (frames, output) = self.chunks[self.active_chunk].get_next_validation_frame()\n",
    "            if frames == None:\n",
    "                # Move to the next chunk\n",
    "                self.active_chunk += 1\n",
    "                if self.active_chunk < len(self.chunks):\n",
    "                    self.chunks[self.active_chunk].move_first_validation_frame()\n",
    "                    return self.get_next_validation_frame()\n",
    "            return (frames, output)\n",
    "        \n",
    "        return (None, None)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Opening training frames from config %s.\" % (data_path))\n",
    "position_rel_indexes = [0, 10] # Predict current rod positions and future position in 10 frames\n",
    "frame_rel_indexes = [0] # Use only current frame as input\n",
    "training = TrainingInput(data_path, position_rel_indexes, frame_rel_indexes, 0.2)\n",
    "\n",
    "'''\n",
    "training.move_first_validation_frame()\n",
    "(frame, position) = training.get_next_validation_frame()\n",
    "count = 0\n",
    "while frame != None:\n",
    "    (frame, position) = training.get_next_validation_frame()\n",
    "    print(\"%i - %i\" % (training.active_chunk, training.chunks[training.active_chunk].current_frame))\n",
    "    count += 1\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Input training frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train our model using CNTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "class TrainingIterator(object):\n",
    "    def __init__(self, input):\n",
    "        self.input = input\n",
    "    \n",
    "    def reset(self):\n",
    "        self.input.move_first_training_frame()\n",
    "    \n",
    "    def __iter__(self):\n",
    "        self.input.move_first_training_frame()\n",
    "        (frames, output) = self.input.get_next_training_frame()\n",
    "        while frames != None:\n",
    "            yield (frames, output)\n",
    "            (frames, output) = self.input.get_next_training_frame()\n",
    "    \n",
    "class ValidationIterator(object):\n",
    "    def __init__(self, input):\n",
    "        self.input = input\n",
    "    \n",
    "    def reset(self):\n",
    "        self.input.move_first_validation_frame()\n",
    "    \n",
    "    def __iter__(self):\n",
    "        self.input.move_first_validation_frame()\n",
    "        (frames, output) = self.input.get_next_validation_frame()\n",
    "        \n",
    "        while frames != None:\n",
    "            yield (frames, output)\n",
    "            (frames, output) = self.input.get_next_validation_frame()\n",
    "            \n",
    "image_height       = training.height\n",
    "image_width        = training.width\n",
    "image_depth        = training.depth\n",
    "image_channels     = training.channels\n",
    "output_size        = training.output_size\n",
    "\n",
    "\n",
    "def TrainGen():\n",
    "    while True:\n",
    "        #print(\"TrainGen restarting training input.\")\n",
    "        training.move_first_training_frame()\n",
    "        (frames, output) = training.get_next_training_frame()\n",
    "        while frames != None:\n",
    "            yield (frames, output)\n",
    "            (frames, output) = training.get_next_training_frame()\n",
    "            \n",
    "def ValidateGen():\n",
    "    while True:\n",
    "        #print(\"Validation restarting training input.\")\n",
    "        training.move_first_validation_frame()\n",
    "        (frames, output) = training.get_next_validation_frame()\n",
    "        while frames != None:\n",
    "            yield (frames, output)\n",
    "            (frames, output) = training.get_next_validation_frame()\n",
    "            \n",
    "def TrainBatchGen(batch_size):\n",
    "    gen = TrainGen()\n",
    "    while True:\n",
    "        # Build the next batch\n",
    "        batch_frames = np.zeros(shape=(batch_size, image_depth, image_height, image_width, image_channels), dtype=np.float32)\n",
    "        batch_outputs = np.zeros(shape=(batch_size, output_size), dtype=np.float32)\n",
    "        for i in range(batch_size):\n",
    "            (frames, output) = next(gen)\n",
    "            output[3] = output[3]-output[0]\n",
    "            output[4] = output[4]-output[1]\n",
    "            output[5] = output[4]-output[2]\n",
    "            batch_frames[i,:,:,:,:] = frames\n",
    "            batch_outputs[i,:] = output\n",
    "            \n",
    "        \n",
    "        #pp.pprint(\"Yielding batch\")\n",
    "        #pp.pprint(batch_outputs)\n",
    "        yield (batch_frames, batch_outputs)\n",
    "        #pp.pprint(\"Yielded batch\")\n",
    "\n",
    "def ValidateBatchGen(batch_size):\n",
    "    gen = ValidateGen()\n",
    "    while True:\n",
    "        # Build the next batch\n",
    "        batch_frames = np.zeros(shape=(batch_size, image_depth, image_height, image_width, image_channels), dtype=np.float32)\n",
    "        batch_outputs = np.zeros(shape=(batch_size, output_size), dtype=np.float32)\n",
    "        for i in range(batch_size):\n",
    "            (frames, output) = next(gen)\n",
    "            output[3] = output[3]-output[0]\n",
    "            output[4] = output[4]-output[1]\n",
    "            output[5] = output[4]-output[2]\n",
    "            batch_frames[i,:,:,:,:] = frames\n",
    "            batch_outputs[i,:] = output\n",
    "        \n",
    "        #pp.pprint(\"Yielding batch\")\n",
    "        #pp.pprint(batch_outputs)\n",
    "        yield (batch_frames, batch_outputs)\n",
    "        #pp.pprint(\"Yielded batch\")\n",
    "    \n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "gen_train = TrainingIterator(training)\n",
    "gen_validation = ValidationIterator(training)\n",
    "\n",
    "WEIGHTS_FNAME = 'mnist_cnn_weights_%i.hdf'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Input shape without batches:'\n",
      "(1, 54, 100, 3)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1 (Conv3D)               (None, 1, 54, 100, 64)    1792      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1, 54, 100, 64)    0         \n",
      "_________________________________________________________________\n",
      "conv1.2 (Conv3D)             (None, 1, 54, 100, 64)    36928     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1, 54, 100, 64)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 1, 27, 50, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv2 (Conv3D)               (None, 1, 27, 50, 64)     36928     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 1, 27, 50, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv2.1 (Conv3D)             (None, 1, 27, 50, 64)     36928     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 1, 27, 50, 64)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 1, 14, 25, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3 (Conv3D)               (None, 1, 14, 25, 64)     36928     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 1, 14, 25, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3.1 (Conv3D)             (None, 1, 14, 25, 64)     36928     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 1, 14, 25, 64)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3 (None, 1, 7, 13, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv4 (Conv3D)               (None, 1, 7, 13, 64)      36928     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 1, 7, 13, 64)      0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 5824)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               745600    \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 981,766\n",
      "Trainable params: 981,766\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "\n",
    "\n",
    "def validate(model, reader, trainer):\n",
    "    # process minibatches and evaluate the model\n",
    "    metric_numer    = 0\n",
    "    metric_denom    = 0\n",
    "    minibatch_index = 0\n",
    "    minibatch_size = 1\n",
    "    \n",
    "    reader.reset()\n",
    "    output_true = np.zeros((reader.size(),2))\n",
    "    output_predicted = np.zeros((reader.size(),2))\n",
    "    current_output = 0\n",
    "    \n",
    "    while reader.has_more():\n",
    "        videos, labels, current_minibatch = reader.next_minibatch(minibatch_size)\n",
    "        \n",
    "        for i in range(minibatch_size):\n",
    "            output_true[current_output,:] = labels[i,:]\n",
    "            \n",
    "            # Use the model to predict the corresponding otuput\n",
    "            video = np.empty(shape=(1, 1, reader.sequence_length, reader.height, reader.width), dtype=np.float32)\n",
    "            video[0,0,:,:,:] = videos[i,:,:,:]\n",
    "            predictions = model.eval({model.arguments[0]:video})\n",
    "            \n",
    "            # Log the result\n",
    "            output_predicted[current_output,:] = predictions\n",
    "            \n",
    "            current_output += 1\n",
    "            \n",
    "        \n",
    "        # minibatch data to be trained with\n",
    "        result = trainer.test_minibatch({input_var : videos, label_var : labels})\n",
    "        #pp.pprint(result)\n",
    "        metric_numer += result * current_minibatch\n",
    "        #print('error rate on an unseen minibatch: {}'.format(metric_numer))\n",
    "        metric_denom += current_minibatch\n",
    "        # Keep track of the number of samples processed so far.\n",
    "        minibatch_index += 1\n",
    "\n",
    "    print(\"Validation Results: Minibatch[1-{}]: errs = {:0.2f}% * {}\".format(minibatch_index+1, (metric_numer*100.0)/metric_denom, metric_denom))\n",
    "    plt.subplot(211)\n",
    "    count = reader.size()\n",
    "    true, predicted = zip(*sorted(zip(output_true[0:count,0], output_predicted[0:count,0])))\n",
    "    plt.plot(range(count),true, range(count),predicted )\n",
    "    plt.ylabel(\"Linear acceleration\")\n",
    "    plt.title(\"First 200 output recordings\")\n",
    "    plt.grid(True)\n",
    "    plt.subplot(212)\n",
    "    true, predicted = zip(*sorted(zip(output_true[0:count,1], output_predicted[0:count,1])))\n",
    "    plt.plot(range(count),true, range(count),predicted, marker='.', markersize = 2, linewidth =0.1, markerfacecolor='black')\n",
    "    plt.ylabel(\"Angular velocity\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# Training options\n",
    "max_epochs = 100\n",
    "\n",
    "# These values must match for both train and test reader.\n",
    "image_height       = training.height\n",
    "image_width        = training.width\n",
    "image_depth        = training.depth\n",
    "image_channels     = training.channels\n",
    "output_size        = training.output_size\n",
    "\n",
    "\n",
    "\n",
    "# Build the model\n",
    "pp.pprint(\"Input shape without batches:\")\n",
    "pp.pprint((image_depth, image_height, image_width, image_channels))\n",
    "model = Sequential([\n",
    "\n",
    "    Conv3D(64,\n",
    "           input_shape=(image_depth, image_height, image_width, image_channels),\n",
    "           data_format=\"channels_last\",\n",
    "           kernel_size = (1, 3, 3),\n",
    "           padding = \"same\",\n",
    "           name = \"conv1\"),\n",
    "    \n",
    "    Activation('relu'),\n",
    "        \n",
    "    Conv3D(64,\n",
    "           data_format=\"channels_last\",\n",
    "           kernel_size = (1, 3, 3),\n",
    "           padding = \"same\",\n",
    "           name = \"conv1.2\"),\n",
    "        \n",
    "    Activation('relu'),\n",
    "        \n",
    "    MaxPooling3D( pool_size=(1, 2, 2),\n",
    "                  padding = \"same\",\n",
    "                  data_format=\"channels_last\"),\n",
    "    \n",
    "    Conv3D(64,\n",
    "           data_format=\"channels_last\",\n",
    "           kernel_size = (1, 3, 3),\n",
    "           padding = \"same\",\n",
    "           name = \"conv2\"),\n",
    "    \n",
    "    Activation('relu'),\n",
    "        \n",
    "    Conv3D(64,\n",
    "           data_format=\"channels_last\",\n",
    "           kernel_size = (1, 3, 3),\n",
    "           padding = \"same\",\n",
    "           name = \"conv2.1\"),\n",
    "    \n",
    "    Activation('relu'),\n",
    "        \n",
    "    MaxPooling3D( pool_size=(1, 2, 2),\n",
    "                  padding = \"same\",\n",
    "                  data_format=\"channels_last\"),\n",
    "        \n",
    "    Conv3D(64,\n",
    "           data_format=\"channels_last\",\n",
    "           kernel_size = (1, 3, 3),\n",
    "           padding = \"same\",\n",
    "           name = \"conv3\"),\n",
    "    \n",
    "    Activation('relu'),\n",
    "        \n",
    "    Conv3D(64,\n",
    "           data_format=\"channels_last\",\n",
    "           kernel_size = (1, 3, 3),\n",
    "           padding = \"same\",\n",
    "           name = \"conv3.1\"),\n",
    "    \n",
    "    Activation('relu'),\n",
    "        \n",
    "    MaxPooling3D( pool_size=(1, 2, 2),\n",
    "                  padding = \"same\",\n",
    "                  data_format=\"channels_last\"),\n",
    "        \n",
    "    Conv3D(64,\n",
    "           data_format=\"channels_last\",\n",
    "           kernel_size = (1, 3, 3),\n",
    "           padding = \"same\",\n",
    "           name = \"conv4\"),\n",
    "    \n",
    "    Activation('relu'),\n",
    "    \n",
    "    Flatten(),\n",
    "    \n",
    "    Dense(128),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    Dense(64),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.5),\n",
    "        \n",
    "    Dense(64),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    Dense(6),\n",
    "])\n",
    "\n",
    "'''\n",
    "model = Sequential([\n",
    "    Conv3D(32,\n",
    "           input_shape=(1, sequence_length, image_height, image_width),\n",
    "           data_format=\"channels_first\",\n",
    "           kernel_size = (3, 3, 3),\n",
    "           padding = \"same\"),\n",
    "    \n",
    "    Activation('relu'),\n",
    "  \n",
    "        \n",
    "    MaxPooling3D( pool_size=(1, 2, 2),\n",
    "                  padding = \"same\",\n",
    "                  data_format=\"channels_first\"),\n",
    "    \n",
    "    Dropout(0.5), \n",
    "    \n",
    "    \n",
    "    \n",
    "    Conv3D(128,\n",
    "           data_format=\"channels_first\",\n",
    "           kernel_size = (1, 3, 3),\n",
    "           padding = \"same\"),\n",
    "    \n",
    "    Activation('relu'),\n",
    "    \n",
    "    MaxPooling3D( pool_size=(1, 2, 2),\n",
    "                  padding = \"same\",\n",
    "                  data_format=\"channels_first\"),\n",
    "    \n",
    "    Dropout(0.5),\n",
    "    \n",
    "        \n",
    "        \n",
    "    Conv3D(32,\n",
    "           data_format=\"channels_first\",\n",
    "           kernel_size = (1, 3, 3),\n",
    "           padding = \"same\"),\n",
    "    \n",
    "    Activation('relu'),\n",
    "    \n",
    "    MaxPooling3D( pool_size=(1, 2, 2),\n",
    "                  padding = \"same\",\n",
    "                  data_format=\"channels_first\"),\n",
    "    \n",
    "    Dropout(0.5),\n",
    "        \n",
    "        \n",
    "    Conv3D(32,\n",
    "           data_format=\"channels_first\",\n",
    "           kernel_size = (1, 3, 3),\n",
    "           padding = \"same\"),\n",
    "    \n",
    "    Activation('relu'),\n",
    "    \n",
    "    MaxPooling3D( pool_size=(2, 2, 2),\n",
    "                  padding = \"same\",\n",
    "                  data_format=\"channels_first\"),\n",
    "    \n",
    "    Dropout(0.5),\n",
    "    \n",
    "    \n",
    "        \n",
    "    Flatten(),\n",
    "    \n",
    "    Dense(512),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.5),\n",
    "        \n",
    "    Dense(64),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    Dense(output_size),\n",
    "])\n",
    "'''\n",
    "\n",
    "# For a multi-class classification problem\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(lr=0.00001),\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model.\n"
     ]
    }
   ],
   "source": [
    "epoch = 22\n",
    "WEIGHTS_FNAME = 'mnist_cnn_weights_%i.hdf'\n",
    "model.load_weights(WEIGHTS_FNAME % epoch)\n",
    "print(\"Loaded model.\")\n",
    "\n",
    "#output_true = labels_validate\n",
    "#output_predicted = model.predict(data_validate, batch_size=32, verbose=1)\n",
    "#print(\"Predicted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\local\\Anaconda3-4.1.1-Windows-x86_64\\lib\\site-packages\\ipykernel\\__main__.py:236: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "C:\\local\\Anaconda3-4.1.1-Windows-x86_64\\lib\\site-packages\\ipykernel\\__main__.py:52: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    }
   ],
   "source": [
    "def plot_validate():\n",
    "    (frames, outputs_true) = next(ValidateBatchGen(2000))\n",
    "    outputs_predicted = model.predict(frames, batch_size=32, verbose=1)\n",
    "    print(\"Predicted.\")\n",
    "    \n",
    "    \n",
    "    pp.pprint(outputs_true)\n",
    "    pp.pprint(outputs_predicted)\n",
    "    \n",
    "    plt.figure(figsize=(5,20))\n",
    "    plt.subplot(611)\n",
    "    \n",
    "    count = 2000\n",
    "    true, predicted = zip(*sorted(zip(outputs_true[0:count,0], outputs_predicted[0:count,0])))\n",
    "    plt.plot(range(count),true, range(count),predicted )\n",
    "    plt.ylabel(\"Rod 1 Position\")\n",
    "    plt.title(\"First 200 output recordings\")\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(612)\n",
    "    true, predicted = zip(*sorted(zip(outputs_true[0:count,1], outputs_predicted[0:count,1])))\n",
    "    plt.plot(range(count),true, range(count),predicted, marker='.', markersize = 2, linewidth =0.1, markerfacecolor='black')\n",
    "    plt.ylabel(\"Rod 2 Position\")\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(613)\n",
    "    true, predicted = zip(*sorted(zip(outputs_true[0:count,2], outputs_predicted[0:count,2])))\n",
    "    plt.plot(range(count),true, range(count),predicted, marker='.', markersize = 2, linewidth =0.1, markerfacecolor='black')\n",
    "    plt.ylabel(\"Rod 3 Position\")\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(614)\n",
    "    true, predicted = zip(*sorted(zip(outputs_true[0:count,3], outputs_predicted[0:count,3])))\n",
    "    plt.plot(range(count),true, range(count),predicted, marker='.', markersize = 2, linewidth =0.1, markerfacecolor='black')\n",
    "    plt.ylabel(\"Rod 1 Position in 1/2 second\")\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(615)\n",
    "    true, predicted = zip(*sorted(zip(outputs_true[0:count,4], outputs_predicted[0:count,4])))\n",
    "    plt.plot(range(count),true, range(count),predicted, marker='.', markersize = 2, linewidth =0.1, markerfacecolor='black')\n",
    "    plt.ylabel(\"Rod 2 Position in 1/2 second\")\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(616)\n",
    "    true, predicted = zip(*sorted(zip(outputs_true[0:count,5], outputs_predicted[0:count,5])))\n",
    "    plt.plot(range(count),true, range(count),predicted, marker='.', markersize = 2, linewidth =0.1, markerfacecolor='black')\n",
    "    plt.ylabel(\"Rod 3 Position in 1/2 second\")\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "plot_validate()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
